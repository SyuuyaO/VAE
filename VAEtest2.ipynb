{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a15844-dc11-4cb4-b802-f2523dc853ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "    def __init__(self, filenames, labels, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.filenames)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        # Select sample\n",
    "        filename = self.filenames[index]\n",
    "        X = Image.open(filename)\n",
    "\n",
    "        if self.transform:\n",
    "            X = self.transform(X)     # transform\n",
    "\n",
    "        y = torch.LongTensor([self.labels[index]])\n",
    "        return X, y\n",
    "\n",
    "## ---------------------- end of Dataloaders ---------------------- ##\n",
    "\n",
    "def conv2D_output_size(img_size, padding, kernel_size, stride):\n",
    "    # compute output shape of conv2D\n",
    "    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),\n",
    "                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int))\n",
    "    return outshape\n",
    "\n",
    "def convtrans2D_output_size(img_size, padding, kernel_size, stride):\n",
    "    # compute output shape of conv2D\n",
    "    outshape = ((img_size[0] - 1) * stride[0] - 2 * padding[0] + kernel_size[0],\n",
    "                (img_size[1] - 1) * stride[1] - 2 * padding[1] + kernel_size[1])\n",
    "    return outshape\n",
    "\n",
    "## ---------------------- ResNet VAE ---------------------- ##\n",
    "\n",
    "class ResNet_VAE(nn.Module):\n",
    "    def __init__(self, fc_hidden1=1024, fc_hidden2=768, drop_p=0.3, CNN_embed_dim=256):\n",
    "        super(ResNet_VAE, self).__init__()\n",
    "\n",
    "        self.fc_hidden1, self.fc_hidden2, self.CNN_embed_dim = fc_hidden1, fc_hidden2, CNN_embed_dim\n",
    "\n",
    "        # CNN architechtures\n",
    "        self.ch1, self.ch2, self.ch3, self.ch4 = 16, 32, 64, 128\n",
    "        self.k1, self.k2, self.k3, self.k4 = (5, 5), (3, 3), (3, 3), (3, 3)      # 2d kernal size\n",
    "        self.s1, self.s2, self.s3, self.s4 = (2, 2), (2, 2), (2, 2), (2, 2)      # 2d strides\n",
    "        self.pd1, self.pd2, self.pd3, self.pd4 = (0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\n",
    "\n",
    "        # encoding components\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(resnet.fc.in_features, self.fc_hidden1)\n",
    "        self.bn1 = nn.BatchNorm1d(self.fc_hidden1, momentum=0.01)\n",
    "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(self.fc_hidden2, momentum=0.01)\n",
    "        # Latent vectors mu and sigma\n",
    "        self.fc3_mu = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)      # output = CNN embedding latent variables\n",
    "        self.fc3_logvar = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)  # output = CNN embedding latent variables\n",
    "\n",
    "        # Sampling vector\n",
    "        self.fc4 = nn.Linear(self.CNN_embed_dim, self.fc_hidden2)\n",
    "        self.fc_bn4 = nn.BatchNorm1d(self.fc_hidden2)\n",
    "        self.fc5 = nn.Linear(self.fc_hidden2, 64 * 4 * 4)\n",
    "        self.fc_bn5 = nn.BatchNorm1d(64 * 4 * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.convTrans6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=self.k4, stride=self.s4,\n",
    "                               padding=self.pd4),\n",
    "            nn.BatchNorm2d(32, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.convTrans7 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=8, kernel_size=self.k3, stride=self.s3,\n",
    "                               padding=self.pd3),\n",
    "            nn.BatchNorm2d(8, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.convTrans8 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=self.k2, stride=self.s2,\n",
    "                               padding=self.pd2),\n",
    "            nn.BatchNorm2d(3, momentum=0.01),\n",
    "            nn.Sigmoid()    # y = (y1, y2, y3) \\in [0 ,1]^3\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.resnet(x)  # ResNet\n",
    "        x = x.view(x.size(0), -1)  # flatten output of conv\n",
    "\n",
    "        # FC layers\n",
    "        x = self.bn1(self.fc1(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(self.fc2(x))\n",
    "        x = self.relu(x)\n",
    "        # x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "        mu, logvar = self.fc3_mu(x), self.fc3_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.relu(self.fc_bn4(self.fc4(z)))\n",
    "        x = self.relu(self.fc_bn5(self.fc5(x))).view(-1, 64, 4, 4)\n",
    "        x = self.convTrans6(x)\n",
    "        x = self.convTrans7(x)\n",
    "        x = self.convTrans8(x)\n",
    "        x = F.interpolate(x, size=(224, 224), mode='bilinear')\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_reconst = self.decode(z)\n",
    "\n",
    "        return x_reconst, z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effbde31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syudi\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\syudi\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU!\n",
      "Epoch 1 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 0.6314\n",
      "\n",
      "Epoch 2 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 0.6504\n",
      "\n",
      "Epoch 3 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 0.3856\n",
      "\n",
      "Epoch 4 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 0.4629\n",
      "\n",
      "Epoch 5 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 0.6875\n",
      "\n",
      "Epoch 6 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 0.9788\n",
      "\n",
      "Epoch 7 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 1.2214\n",
      "\n",
      "Epoch 8 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 1.3547\n",
      "\n",
      "Epoch 9 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 1.5552\n",
      "\n",
      "Epoch 10 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 1.4206\n",
      "\n",
      "Epoch 11 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 1.2266\n",
      "\n",
      "Epoch 12 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 1.0550\n",
      "\n",
      "Epoch 13 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 0.8736\n",
      "\n",
      "Epoch 14 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 0.8496\n",
      "\n",
      "Epoch 15 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 0.7636\n",
      "\n",
      "Epoch 16 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 0.7512\n",
      "\n",
      "Epoch 17 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 0.6931\n",
      "\n",
      "Epoch 18 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 0.6858\n",
      "\n",
      "Epoch 19 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 0.5674\n",
      "\n",
      "Epoch 20 model saved!\n",
      "\n",
      "Test set (32 samples): Average loss: 0.5577\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "#import torchvision.models as models\n",
    "#import cv2\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data\n",
    "#import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as pltd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.utils import save_image\n",
    "#import pickle\n",
    "\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# EncoderCNN architecture\n",
    "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 1024\n",
    "CNN_embed_dim = 256     # latent dim extracted by 2D CNN\n",
    "res_size = 224        # ResNet image size\n",
    "dropout_p = 0.2       # dropout probability\n",
    "\n",
    "\n",
    "# training parameters\n",
    "epochs = 20        # training epochs\n",
    "batch_size = 16\n",
    "learning_rate = 1e-4\n",
    "log_interval = 10   # interval for displaying training info\n",
    "\n",
    "# save model\n",
    "save_model_path = './results_cifar10'\n",
    "\n",
    "def check_mkdir(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = F.mse_loss(recon_x, x, reduction='mean')\n",
    "    #MSE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + KLD\n",
    "\n",
    "\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    all_y, all_z, all_mu, all_logvar = [], [], [], []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device).view(-1, )\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        X_reconst, z, mu, logvar = model(X)  # VAE\n",
    "        loss = loss_function(X_reconst, X, mu, logvar)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        all_y.extend(y.data.cpu().numpy())\n",
    "        all_z.extend(z.data.cpu().numpy())\n",
    "        all_mu.extend(mu.data.cpu().numpy())\n",
    "        all_logvar.extend(logvar.data.cpu().numpy())\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "\n",
    "    all_y = np.stack(all_y, axis=0)\n",
    "    all_z = np.stack(all_z, axis=0)\n",
    "    all_mu = np.stack(all_mu, axis=0)\n",
    "    all_logvar = np.stack(all_logvar, axis=0)\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(model.state_dict(), os.path.join(save_model_path, 'model_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "\n",
    "    return X_reconst.data.cpu().numpy(), all_y, all_z, all_mu, all_logvar, losses\n",
    "\n",
    "\n",
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y, all_z, all_mu, all_logvar = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device).view(-1, )\n",
    "            X_reconst, z, mu, logvar = model(X)\n",
    "\n",
    "            loss = loss_function(X_reconst, X, mu, logvar)\n",
    "            test_loss += loss.item()  # sum up batch loss\n",
    "\n",
    "            all_y.extend(y.data.cpu().numpy())\n",
    "            all_z.extend(z.data.cpu().numpy())\n",
    "            all_mu.extend(mu.data.cpu().numpy())\n",
    "            all_logvar.extend(logvar.data.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    all_y = np.stack(all_y, axis=0)\n",
    "    all_z = np.stack(all_z, axis=0)\n",
    "    all_mu = np.stack(all_mu, axis=0)\n",
    "    all_logvar = np.stack(all_logvar, axis=0)\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}\\n'.format(len(test_loader.dataset), test_loss))\n",
    "    return X_reconst.data.cpu().numpy(), all_y, all_z, all_mu, all_logvar, test_loss\n",
    "\n",
    "def show_generated_images(images):\n",
    "    import matplotlib.pyplot as plt\n",
    "    grid_img = vutils.make_grid(images.cpu(), nrow=10, normalize=True)\n",
    "    np_img = grid_img.numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(np_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_single_image(image):\n",
    "    import matplotlib.pyplot as plt\n",
    "    # 画像テンソルを NumPy 配列に変換\n",
    "    np_img = image.cpu().numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # 画像を表示\n",
    "    plt.imshow(np_img)\n",
    "    plt.axis('off')  # 軸を表示しない\n",
    "    plt.show()\n",
    "\n",
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "# Data loading parameters\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 2, 'pin_memory': True} if use_cuda else {}\n",
    "# transform = transforms.Compose([transforms.Resize([res_size, res_size]),\n",
    "#                                 transforms.ToTensor(),\n",
    "#                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize([res_size, res_size]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])])\n",
    "\n",
    "\n",
    "\n",
    "# 訓練データとテストデータのパス\n",
    "train_data_path = 'C:\\\\Users\\\\syudi\\\\neuralnet\\\\bubblejet\\\\bubblejet\\\\datasetVAEtrain'\n",
    "test_data_path = 'C:\\\\Users\\\\syudi\\\\neuralnet\\\\bubblejet\\\\bubblejet\\\\datasetVAEtest'\n",
    "\n",
    "# データセットの作成\n",
    "train_dataset = datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_data_path, transform=transform)\n",
    "\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create model\n",
    "resnet_vae = ResNet_VAE(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "\n",
    "print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n",
    "model_params = list(resnet_vae.parameters())\n",
    "optimizer = torch.optim.Adam(model_params, lr=learning_rate)\n",
    "\n",
    "\n",
    "# record training process\n",
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "check_mkdir(save_model_path)\n",
    "\n",
    "# start training\n",
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    X_reconst_train, y_train, z_train, mu_train, logvar_train, train_losses = train(log_interval, resnet_vae, device, train_loader, optimizer, epoch)\n",
    "    X_reconst_test, y_test, z_test, mu_test, logvar_test, epoch_test_loss = validation(resnet_vae, device, optimizer, valid_loader)\n",
    "\n",
    "    # save results\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "\n",
    "    # save all train test results\n",
    "    A = np.array(epoch_train_losses)\n",
    "    C = np.array(epoch_test_losses)\n",
    "    \n",
    "    np.save(os.path.join(save_model_path, 'ResNet_VAE_training_loss.npy'), A)\n",
    "    np.save(os.path.join(save_model_path, 'y_cifar10_train_epoch{}.npy'.format(epoch + 1)), y_train)\n",
    "    np.save(os.path.join(save_model_path, 'z_cifar10_train_epoch{}.npy'.format(epoch + 1)), z_train)\n",
    "    \n",
    "    \n",
    "    ###画像表示部###\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        resnet_vae.eval()  # Switch to evaluation mode for generating images\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(1, CNN_embed_dim).to(device)  # Generate random sample\n",
    "            sample_image = resnet_vae.decode(sample)  # Decode the latent vectors to images\n",
    "            # 画像を保存または表示するためのコードを追加\n",
    "            save_image(sample_image, os.path.join(save_model_path, f'sample_epoch_{epoch + 1}.png'))\n",
    "            # 生成された画像を表示\n",
    "            #show_single_image(sample_image)\n",
    "\n",
    "\n",
    "    ##############\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4ef92-371b-4c2e-bed6-12462205d0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725b1ac-cf66-4c06-bbeb-e2dc460f73e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
